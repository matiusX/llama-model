{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9ea0cf-04cf-4b5b-8166-17f269bf31dd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7051a631-622f-4627-b708-81d2827f47cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Requirement already satisfied: pandas in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: requests in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: torch in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Collecting fairscale\n",
      "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m835.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from fairscale) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from fairscale) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->fairscale) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->fairscale) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/matius/anaconda3/envs/master/lib/python3.12/site-packages (from sympy->torch>=1.8.0->fairscale) (1.3.0)\n",
      "Building wheels for collected packages: fairscale\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332105 sha256=47d921a98279d543d75cc894995327c80ae7992e49fac1ecf0ebaf4d35612e42\n",
      "  Stored in directory: /home/matius/.cache/pip/wheels/5a/88/aa/d84b2cf1bad6b273cbf661640141a82c7b9f496e024f80aac0\n",
      "Successfully built fairscale\n",
      "Installing collected packages: fairscale\n",
      "Successfully installed fairscale-0.4.13\n"
     ]
    }
   ],
   "source": [
    "#realizar os \"pip install\"\n",
    "%pip install sentencepiece\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install requests\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install fairscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b037491-7498-4276-b88e-54de837851d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import sentencepiece as spm\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a642df-b90f-4a77-b11c-72493b377669",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd142b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9733377d-4d2b-4a18-9fac-131b9505e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 130\n",
    "BATCH_SIZE = 32\n",
    "CONTEXT_WINDOW = 16\n",
    "EPOCHS = 1000\n",
    "DIM = 128\n",
    "LOG_INTERVAL = 10\n",
    "HEADS = 8\n",
    "LAYERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebf76e0-55ce-42da-8bd3-8c5b5abe295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    tinyshakespeare = response.text\n",
    "else:\n",
    "    print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb2fd2d-9258-4df7-b389-1c21197c4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyshakespeare_list = tinyshakespeare.split(\"\\n\")\n",
    "tinyshakespeare_list = [i for i in tinyshakespeare_list if i != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c39294-569f-4b53-a244-e5abad6a2b85",
   "metadata": {},
   "source": [
    "### Tokenizer - SentencePiece\n",
    "\n",
    "\n",
    "- paper: https://arxiv.org/pdf/1808.06226\n",
    "- lib: https://github.com/google/sentencepiece#train-sentencepiece-model\n",
    "- BPE based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ac6ad-1650-46c1-9681-5c6abba50195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#JUST RUN ONE TIME\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator = iter(tinyshakespeare_list),\n",
    "    model_prefix = \"tinyshakespeare_model\",\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    character_coverage = 1.0,\n",
    "    model_type = \"bpe\",\n",
    "    pad_id = 0,\n",
    "    unk_id = 1,\n",
    "    bos_id = 2,\n",
    "    eos_id = 3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff20b79-9876-4ab5-884e-776d8304a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file = \"tinyshakespeare_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad610863-f986-4add-a6e1-b508b99682b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thy deed, inhuman and unnatural, --> [39, 71, 80, 25, 67, 67, 77, 81, 58, 71, 78, 79, 50, 46, 66, 78, 74, 74, 53, 78, 73, 70, 76, 81]\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "sentence = random.choice(tinyshakespeare_list)\n",
    "print(sentence, \"-->\", sp.Encode(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef80ac4-e85a-4e38-a0d8-6aa060f15e23",
   "metadata": {},
   "source": [
    "### Train, test, validation split in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca33dd4-1078-45d3-8044-78530e349953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 66, 114,  64,  ...,  93,  48,  90]) torch.Size([734937])\n"
     ]
    }
   ],
   "source": [
    "dataset_tensor = torch.tensor(sp.Encode(tinyshakespeare))\n",
    "print(dataset_tensor, dataset_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "370a41d7-856a-4287-a96a-128a26e1620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_train(dataset, batch_size, context_window):\n",
    "    train_data = dataset[:int(.7 * len(dataset))]\n",
    "    ix = torch.randint(0, train_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([train_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([train_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = get_batch_train(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e871b05b-61f7-43ac-a99f-ac6f72b75f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16]) torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b84d1d2-7c2a-4ebe-bbf1-ae340c17128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_val(dataset, batch_size, context_window):\n",
    "    val_data = dataset[int(.7 * len(dataset)): int(.85 * len(dataset))]\n",
    "    ix = torch.randint(0, val_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([val_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([val_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y\n",
    "\n",
    "x_val, y_val = get_batch_val(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5999725d-c487-4367-b640-eb81bcb66bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16]) torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1378ca8-95c0-44db-b201-ea5c98c7fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_test(dataset, batch_size, context_window):\n",
    "    test_data = dataset[int(.85 * len(dataset)): len(dataset)]\n",
    "    ix = torch.randint(0, test_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([test_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([test_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y\n",
    "\n",
    "x_test, y_test = get_batch_test(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf91de7a-d3b0-4b64-b2f2-179e40ceed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16]) torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b0d8e-f307-4497-9ee0-f012e2c480a1",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cc4ae8b-4b14-4cf1-bbf8-c6797bba3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_loss(model):    \n",
    "    model.eval()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(EPOCHS):\n",
    "        x_train, y_train = get_batch_train(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)\n",
    "        _, train_loss = model(x_train, y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        \n",
    "        x_val, y_val = get_batch_val(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)\n",
    "        _, val_loss = model(x_val, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    losses_dict = {\"train\": np.mean(train_losses), \"val\": np.mean(val_losses)}\n",
    "    return losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "788d1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_accuracy(model):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        x_val, y_val = get_batch_val(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)\n",
    "        \n",
    "        logits = model(x_val)\n",
    "        \n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        correct_predictions += (predicted_labels == y_val).sum().item()\n",
    "        total_predictions += y_val.numel()\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86f9f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_perplexity(model):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        x_val, y_val = get_batch_val(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)\n",
    "        \n",
    "        _, val_loss = model(x_val, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    mean_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    perplexity = np.exp(mean_val_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da975265-ecc5-4009-b16a-1ff30dcab222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrumar\n",
    "def train(model, optimizer, checkpoint_path=\"/home/matius/Documents/mestrado/nlp/checkpoints\"):\n",
    "    losses = []\n",
    "    accs = []\n",
    "    perps = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()        \n",
    "        x_train, y_train = get_batch_train(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)\n",
    "        logits, loss = model(x_train, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % LOG_INTERVAL == 0:\n",
    "            current_loss = calculate_loss(model)\n",
    "            current_accuracy = calculate_accuracy(model)\n",
    "            current_perplexity = calculate_perplexity(model)\n",
    "\n",
    "            losses.append(current_loss)\n",
    "            accs.append(current_accuracy)\n",
    "            perps.append(current_perplexity)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': current_loss,\n",
    "                'accuracy': current_accuracy,\n",
    "                'perplexity': current_perplexity\n",
    "            }, f\"{checkpoint_path}/checkpoint_epoch_{epoch}.pth\")\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Loss - {current_loss['val']}, Accuracy - {current_accuracy}, Perplexity - {current_perplexity}\")\n",
    "\n",
    "\n",
    "    print(\"validation Loss: \", losses[-1]['val'])\n",
    "    print(\"validation Accuracy: \", accs[-1])\n",
    "    print(\"validation Perplexity: \", perps[-1])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e1c40-810b-43ce-bc54-6c8f3c9a94aa",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "- RMSNorm\n",
    "- SwiGLU\n",
    "- Rotary Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4fa5a-2f86-487f-b036-4706dc8861b3",
   "metadata": {},
   "source": [
    "### RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1287856-cfaf-4e45-8747-fc59ce4a1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.register_parameter(\"scale\", torch.nn.Parameter(torch.ones(layer_shape)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.scale[:x.shape[1], :].unsqueeze(0) * ((torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5).unsqueeze(-1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa51e02f-6366-4d93-a5eb-79103e1add99",
   "metadata": {},
   "source": [
    "### Rotatory Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9883a9d-6f27-4e3a-9f50-c3b047cee4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*(i - 1) / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2003907-6ada-44ef-a6d7-a5dbf5046008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEAttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w_q = nn.Linear(DIM, DIM, bias=False)\n",
    "        self.w_k = nn.Linear(DIM, DIM, bias=False)\n",
    "        self.w_v = nn.Linear(DIM, DIM, bias=False)\n",
    "\n",
    "        self.R = get_rotary_matrix(CONTEXT_WINDOW, DIM)\n",
    "\n",
    "    def get_rotary_matrix(context_window, embedding_dim):\n",
    "        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "        for position in range(context_window):\n",
    "            for i in range(embedding_dim//2):\n",
    "                theta = 10000. ** (-2.*(i - 1) / embedding_dim)\n",
    "                m_theta = position * theta\n",
    "                R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "                R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "                R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "                R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "        return R\n",
    "    \n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        b,m,d = x.shape\n",
    "        \n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q_rotated = (torch.bmm(q.transpose(0,1), self.R[:m])).transpose(0,1)\n",
    "        k_rotated = (torch.bmm(k.transpose(0,1), self.R[:m])).transpose(0,1)\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(\n",
    "            q_rotated,k_rotated,v,dropout_p =.1\n",
    "        )\n",
    "\n",
    "        if return_attn_weights:\n",
    "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1,2)) / np.sqrt(d)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            return activations, attn_weights\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2889b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEAttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w_q = nn.Linear(DIM, DIM, bias=False)\n",
    "        self.w_k = nn.Linear(DIM, DIM, bias=False)\n",
    "        self.w_v = nn.Linear(DIM, DIM, bias=False)\n",
    "\n",
    "        self.R = get_rotary_matrix(CONTEXT_WINDOW, DIM)\n",
    "\n",
    "    def get_rotary_matrix(context_window, embedding_dim):\n",
    "        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "        for position in range(context_window):\n",
    "            for i in range(embedding_dim//2):\n",
    "                theta = 10000. ** (-2.*(i - 1) / embedding_dim)\n",
    "                m_theta = position * theta\n",
    "                R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "                R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "                R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "                R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "        return R\n",
    "    \n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        b,m,d = x.shape\n",
    "        \n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q_rotated = (torch.bmm(q.transpose(0,1), self.R[:m])).transpose(0,1)\n",
    "        k_rotated = (torch.bmm(k.transpose(0,1), self.R[:m])).transpose(0,1)\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(\n",
    "            q_rotated,k_rotated,v,dropout_p =.1, is_causal=True\n",
    "        )\n",
    "\n",
    "        if return_attn_weights:\n",
    "            attn_mask = torch.tril(torch.ones((m,m)), diagonal=0)\n",
    "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1,2)) / np.sqrt(d) + attn_mask\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            return activations, attn_weights\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb39d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEMultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            RoPEAttentionHead() for _ in range(HEADS)\n",
    "        ])\n",
    "        self.linear = nn.Linear(HEADS * DIM, DIM)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads = [h(x) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c655f-0cd4-4d42-8a99-eff3e102e3d9",
   "metadata": {},
   "source": [
    "### SwiGlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8fcd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.linear_gate = nn.Linear(size, size)\n",
    "        self.linear = nn.Linear(size, size)\n",
    "        self.beta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.ones(1))\n",
    "        self.register_parameter(\"beta\", self.beta)\n",
    "\n",
    "    def forward(self, x): \n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e783dcc-ed11-44e9-8183-40c5a01721bb",
   "metadata": {},
   "source": [
    "## Llama block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94dd960e-3768-42a7-ad0b-cf3e0bc9dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rms = RMSNorm((CONTEXT_WINDOW, DIM))\n",
    "        \n",
    "        self.attention = RoPEMultiheadAttention()\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(DIM, DIM),\n",
    "            SwiGLU(DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rms(x)             #RMS NORMALIZATION \n",
    "        x = x + self.attention(x)   #Self attention\n",
    "\n",
    "        x = self.rms(x)             #RMS NORMALIZATION\n",
    "        x = x + self.feedforward(x) #Feed Foward: SwiGlu\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73cafe6b-0009-4a95-987f-1af3864017e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, DIM)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock()) for i in range(LAYERS)])\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(DIM, DIM),\n",
    "            SwiGLU(DIM),\n",
    "            nn.Linear(DIM, VOCAB_SIZE),\n",
    "        )\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embeddings(idx)\n",
    "        x = self.llama_blocks(x)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, VOCAB_SIZE), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a13865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params: 2386951\n",
      "Epoch 0: Loss - 169.458105178833, Accuracy - 0.0261015625, Perplexity - 3.24933467426742e+73\n",
      "Epoch 10: Loss - 4.858779852390289, Accuracy - 0.015146484375, Perplexity - 128.89264798630532\n",
      "Epoch 20: Loss - 4.853356916904449, Accuracy - 0.015083984375, Perplexity - 128.18218287019613\n",
      "Epoch 30: Loss - 4.847505692005157, Accuracy - 0.015103515625, Perplexity - 127.40874866316938\n",
      "Epoch 40: Loss - 4.841072518348694, Accuracy - 0.015080078125, Perplexity - 126.58874431212625\n",
      "Epoch 50: Loss - 4.833008142471313, Accuracy - 0.015234375, Perplexity - 125.59026871441068\n",
      "Epoch 60: Loss - 4.822472419738769, Accuracy - 0.01528125, Perplexity - 124.27940176518239\n",
      "Epoch 70: Loss - 4.806418401241302, Accuracy - 0.043193359375, Perplexity - 122.2781040302821\n",
      "Epoch 80: Loss - 4.781144430160523, Accuracy - 0.042595703125, Perplexity - 119.2811085436558\n",
      "Epoch 90: Loss - 4.740661253452301, Accuracy - 0.04304296875, Perplexity - 114.49086018207365\n",
      "Epoch 100: Loss - 4.671450594902039, Accuracy - 0.04294140625, Perplexity - 106.8545762526441\n",
      "Epoch 110: Loss - 4.557099345684051, Accuracy - 0.042951171875, Perplexity - 95.45364751113775\n",
      "Epoch 120: Loss - 4.513688383579254, Accuracy - 0.043501953125, Perplexity - 91.50318184308883\n",
      "Epoch 130: Loss - 4.494308381080628, Accuracy - 0.05564453125, Perplexity - 89.77005057444444\n",
      "Epoch 140: Loss - 4.483301341056824, Accuracy - 0.055716796875, Perplexity - 88.61659595295475\n",
      "Epoch 150: Loss - 4.480189060688018, Accuracy - 0.055685546875, Perplexity - 88.32747052262069\n",
      "Epoch 160: Loss - 4.47848773765564, Accuracy - 0.05557421875, Perplexity - 88.20762296991698\n",
      "Epoch 170: Loss - 4.478817452907562, Accuracy - 0.05581640625, Perplexity - 88.01689471191378\n",
      "Epoch 180: Loss - 4.475634211063385, Accuracy - 0.0560625, Perplexity - 87.8274014915356\n",
      "Epoch 190: Loss - 4.474802247047425, Accuracy - 0.05571875, Perplexity - 87.93560749114592\n",
      "Epoch 200: Loss - 4.474877115726471, Accuracy - 0.055841796875, Perplexity - 87.71128045624388\n",
      "Epoch 210: Loss - 4.473865283489228, Accuracy - 0.055267578125, Perplexity - 87.67339919183762\n",
      "Epoch 220: Loss - 4.471398812770843, Accuracy - 0.05616015625, Perplexity - 87.57937178953735\n",
      "Epoch 230: Loss - 4.471776329994202, Accuracy - 0.055861328125, Perplexity - 87.60354849622829\n",
      "Epoch 240: Loss - 4.472202696800232, Accuracy - 0.0555703125, Perplexity - 87.48978654476458\n",
      "Epoch 250: Loss - 4.474342276573181, Accuracy - 0.05608984375, Perplexity - 87.60131352209748\n",
      "Epoch 260: Loss - 4.470425203800201, Accuracy - 0.0558671875, Perplexity - 87.4869615467555\n",
      "Epoch 270: Loss - 4.47057101392746, Accuracy - 0.055541015625, Perplexity - 87.42272739024453\n",
      "Epoch 280: Loss - 4.4684478464126585, Accuracy - 0.055818359375, Perplexity - 87.33903974406458\n",
      "Epoch 290: Loss - 4.47205204629898, Accuracy - 0.056169921875, Perplexity - 87.46353018288893\n",
      "Epoch 300: Loss - 4.47042732334137, Accuracy - 0.056001953125, Perplexity - 87.41607049159036\n",
      "Epoch 310: Loss - 4.4746501836776735, Accuracy - 0.055404296875, Perplexity - 87.67642097685862\n",
      "Epoch 320: Loss - 4.46943891620636, Accuracy - 0.0556875, Perplexity - 87.42757177651461\n",
      "Epoch 330: Loss - 4.471200886249542, Accuracy - 0.055677734375, Perplexity - 87.502857097183\n",
      "Epoch 340: Loss - 4.472122393608093, Accuracy - 0.055849609375, Perplexity - 87.48129276093597\n",
      "Epoch 350: Loss - 4.473696469306946, Accuracy - 0.05580859375, Perplexity - 87.42488673099892\n",
      "Epoch 360: Loss - 4.47376754617691, Accuracy - 0.055837890625, Perplexity - 87.59124531647716\n",
      "Epoch 370: Loss - 4.467496572494507, Accuracy - 0.05598828125, Perplexity - 87.38902259375882\n",
      "Epoch 380: Loss - 4.469488430976868, Accuracy - 0.0555703125, Perplexity - 87.42928553300203\n",
      "Epoch 390: Loss - 4.469564730644226, Accuracy - 0.055814453125, Perplexity - 87.31016404542643\n",
      "Epoch 400: Loss - 4.467379192829132, Accuracy - 0.055615234375, Perplexity - 87.23363809134156\n",
      "Epoch 410: Loss - 4.46985192489624, Accuracy - 0.055490234375, Perplexity - 87.1811348177817\n",
      "Epoch 420: Loss - 4.4696653327941895, Accuracy - 0.05619140625, Perplexity - 87.24124321383141\n",
      "Epoch 430: Loss - 4.467924469947815, Accuracy - 0.055943359375, Perplexity - 86.99935213988978\n",
      "Epoch 440: Loss - 4.4683497552871705, Accuracy - 0.0557578125, Perplexity - 87.22023785202383\n",
      "Epoch 450: Loss - 4.469293005943299, Accuracy - 0.055884765625, Perplexity - 87.26265745925596\n",
      "Epoch 460: Loss - 4.46784251499176, Accuracy - 0.055708984375, Perplexity - 86.85464732258582\n",
      "Epoch 470: Loss - 4.4694577870368954, Accuracy - 0.055484375, Perplexity - 87.37671087773211\n",
      "Epoch 480: Loss - 4.46722679567337, Accuracy - 0.055357421875, Perplexity - 87.18215473168058\n",
      "Epoch 490: Loss - 4.469234353542328, Accuracy - 0.055880859375, Perplexity - 87.10801584973059\n",
      "Epoch 500: Loss - 4.470348662376404, Accuracy - 0.05576171875, Perplexity - 87.16051532010366\n",
      "Epoch 510: Loss - 4.469053965091705, Accuracy - 0.055345703125, Perplexity - 87.33014458806183\n",
      "Epoch 520: Loss - 4.470405468463897, Accuracy - 0.0556953125, Perplexity - 87.23389132961876\n",
      "Epoch 530: Loss - 4.472385866165161, Accuracy - 0.05604296875, Perplexity - 87.12531055642755\n",
      "Epoch 540: Loss - 4.4694074120521545, Accuracy - 0.055728515625, Perplexity - 87.3308167383344\n",
      "Epoch 550: Loss - 4.469836386203766, Accuracy - 0.05549609375, Perplexity - 86.9948720926075\n",
      "Epoch 560: Loss - 4.467711858749389, Accuracy - 0.055671875, Perplexity - 87.41047540555759\n",
      "Epoch 570: Loss - 4.466917342185974, Accuracy - 0.05579296875, Perplexity - 87.11582810540071\n",
      "Epoch 580: Loss - 4.4687431473732, Accuracy - 0.056150390625, Perplexity - 87.21138232523266\n",
      "Epoch 590: Loss - 4.469434727668762, Accuracy - 0.055326171875, Perplexity - 87.42450433299813\n",
      "Epoch 600: Loss - 4.466250173091888, Accuracy - 0.05580859375, Perplexity - 87.0586770979319\n",
      "Epoch 610: Loss - 4.469128870964051, Accuracy - 0.055521484375, Perplexity - 87.14406879258054\n",
      "Epoch 620: Loss - 4.468897793292999, Accuracy - 0.056072265625, Perplexity - 87.51081409242961\n",
      "Epoch 630: Loss - 4.467792912960053, Accuracy - 0.05521484375, Perplexity - 87.2420625270492\n",
      "Epoch 640: Loss - 4.468015439510346, Accuracy - 0.055439453125, Perplexity - 87.32954785654387\n",
      "Epoch 650: Loss - 4.466173994541168, Accuracy - 0.055236328125, Perplexity - 87.31207251065055\n",
      "Epoch 660: Loss - 4.467954704284668, Accuracy - 0.055359375, Perplexity - 87.28635366226531\n",
      "Epoch 670: Loss - 4.4692712845802305, Accuracy - 0.055392578125, Perplexity - 87.3565183213472\n",
      "Epoch 680: Loss - 4.465977133750916, Accuracy - 0.0561953125, Perplexity - 87.1820148430529\n",
      "Epoch 690: Loss - 4.469448421478272, Accuracy - 0.0555390625, Perplexity - 87.21778271294299\n",
      "Epoch 700: Loss - 4.470955973625183, Accuracy - 0.055419921875, Perplexity - 87.40858446224314\n",
      "Epoch 710: Loss - 4.469347177505493, Accuracy - 0.05599609375, Perplexity - 87.2348036257751\n",
      "Epoch 720: Loss - 4.470471522331238, Accuracy - 0.05537890625, Perplexity - 87.30289464882897\n",
      "Epoch 730: Loss - 4.469802432060241, Accuracy - 0.055822265625, Perplexity - 87.04310960790058\n",
      "Epoch 740: Loss - 4.466125435352326, Accuracy - 0.05548828125, Perplexity - 87.12371883171899\n",
      "Epoch 750: Loss - 4.466037197113037, Accuracy - 0.055919921875, Perplexity - 87.288372946719\n",
      "Epoch 760: Loss - 4.468424077510834, Accuracy - 0.055705078125, Perplexity - 87.25538627631849\n",
      "Epoch 770: Loss - 4.468538602352142, Accuracy - 0.05589453125, Perplexity - 87.44668939284732\n",
      "Epoch 780: Loss - 4.465915424346924, Accuracy - 0.0555, Perplexity - 87.2571493326405\n",
      "Epoch 790: Loss - 4.466528355121612, Accuracy - 0.055552734375, Perplexity - 87.21248406022664\n",
      "Epoch 800: Loss - 4.4695005102157594, Accuracy - 0.05530859375, Perplexity - 87.37434936790972\n",
      "Epoch 810: Loss - 4.466292475223542, Accuracy - 0.056001953125, Perplexity - 87.1812234892062\n",
      "Epoch 820: Loss - 4.469673675537109, Accuracy - 0.056087890625, Perplexity - 87.27683345801658\n",
      "Epoch 830: Loss - 4.4667099056243895, Accuracy - 0.055455078125, Perplexity - 87.22735395774814\n",
      "Epoch 840: Loss - 4.468777102947235, Accuracy - 0.0556640625, Perplexity - 87.16583567063871\n",
      "Epoch 850: Loss - 4.466415491580963, Accuracy - 0.055650390625, Perplexity - 87.15377054439038\n",
      "Epoch 860: Loss - 4.466049732208252, Accuracy - 0.05555078125, Perplexity - 87.207290644967\n",
      "Epoch 870: Loss - 4.46755875825882, Accuracy - 0.055806640625, Perplexity - 87.15330571765209\n",
      "Epoch 880: Loss - 4.46891444158554, Accuracy - 0.055421875, Perplexity - 87.05203584097546\n",
      "Epoch 890: Loss - 4.467106063365936, Accuracy - 0.055578125, Perplexity - 87.3701805922537\n",
      "Epoch 900: Loss - 4.468428663730621, Accuracy - 0.055849609375, Perplexity - 87.21652246028849\n",
      "Epoch 910: Loss - 4.471463325977325, Accuracy - 0.05579296875, Perplexity - 87.25862911418714\n",
      "Epoch 920: Loss - 4.467946526527405, Accuracy - 0.055673828125, Perplexity - 87.38519367406634\n",
      "Epoch 930: Loss - 4.46963402223587, Accuracy - 0.055814453125, Perplexity - 87.38674046441984\n",
      "Epoch 940: Loss - 4.47185004901886, Accuracy - 0.0558828125, Perplexity - 87.24343963123084\n",
      "Epoch 950: Loss - 4.467938299179077, Accuracy - 0.05507421875, Perplexity - 87.33724242457853\n",
      "Epoch 960: Loss - 4.4698785161972046, Accuracy - 0.055623046875, Perplexity - 87.33004098218547\n",
      "Epoch 970: Loss - 4.467256767749786, Accuracy - 0.055439453125, Perplexity - 87.25744399710689\n",
      "Epoch 980: Loss - 4.466356420993805, Accuracy - 0.0557890625, Perplexity - 87.19972045730626\n",
      "Epoch 990: Loss - 4.469351182460785, Accuracy - 0.05590234375, Perplexity - 87.31600547944738\n",
      "validation Loss:  4.469351182460785\n",
      "validation Accuracy:  0.05590234375\n",
      "validation Perplexity:  87.31600547944738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama = Llama()\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38937444",
   "metadata": {},
   "source": [
    "## Test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfb7d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_accuracy_test(model):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        x_test, y_test = get_batch_test(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)\n",
    "        \n",
    "        logits = model(x_test)\n",
    "        \n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        correct_predictions += (predicted_labels == y_test).sum().item()\n",
    "        total_predictions += y_test.numel()\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ebb026c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_perplexity(model):\n",
    "    model.eval()\n",
    "    perp_sum = 0\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        x_test, y_test = get_batch_test(dataset_tensor, BATCH_SIZE, CONTEXT_WINDOW)\n",
    "        outputs = model(x_test)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), y_test.view(-1), reduction='mean')\n",
    "        perplexity = torch.exp(loss)\n",
    "        perp_sum += perplexity.item()\n",
    "    \n",
    "    perplexity_final = perp_sum/EPOCHS\n",
    "    return perplexity_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3237de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()  # Set the model to evaluation mode    \n",
    "    with torch.no_grad():  # Disable gradient calculation        \n",
    "        test_accuracy = calculate_accuracy_test(model)\n",
    "        test_perplexity = calculate_perplexity(model)\n",
    "        \n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Test Perplexity: {test_perplexity}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'perplexity': test_perplexity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "810a71a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.056173828125\n",
      "Test Perplexity: 87.69882926177978\n"
     ]
    }
   ],
   "source": [
    "llama_test = test(llama)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
